{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "- 파서 (html, xml 의 형태로 내려온 데이터를 원하는 요소만 찾기 위해 필요)\n",
    "- requests + bs4 : 이 조합으로 주로 크롤링\n",
    "- 파서 종류\n",
    "    - html.parser (두번째 속도)\n",
    "    - lxml (속도가 가장 빠름) : 설치가 필요 pip install lxml\n",
    "    - html5lib (가장 느림)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>[속보]김호중, 구속심사 법원 도착…취재진 질문에 \"죄송하다\"</title>\n",
      "<h3 class=\"tit_view\" data-translation=\"true\">[속보]김호중, 구속심사 법원 도착…취재진 질문에 \"죄송하다\"</h3>\n",
      "[속보]김호중, 구속심사 법원 도착…취재진 질문에 \"죄송하다\"\n",
      "{'class': ['tit_view'], 'data-translation': 'true'}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://v.daum.net/v/20240524110648466\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url)\n",
    "    # soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    # print(soup)\n",
    "\n",
    "    # 요소 접근\n",
    "    # 1. 태그명 사용\n",
    "    print(soup.title)\n",
    "    print(soup.h3)\n",
    "    # get_text() : 태그의 텍스트 추출\n",
    "    print(soup.title.get_text())\n",
    "    # attrs : 태그 속성 추출\n",
    "    print(soup.h3.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title <title>The Dormouse's story</title>\n",
      "title content The Dormouse's story\n",
      "title content The Dormouse's story\n",
      "title content <head>\n",
      "<title>The Dormouse's story</title>\n",
      "</head>\n",
      "==========\n",
      "p <p class=\"title\">\n",
      "<b> The Dormouse's story </b>\n",
      "</p>\n",
      "p The Dormouse's story\n",
      "p {'class': ['title']}\n",
      "p ['title']\n",
      "p <b> The Dormouse's story </b>\n",
      "p The Dormouse's story\n",
      "p The Dormouse's story\n"
     ]
    }
   ],
   "source": [
    "url = \"./story.html\"\n",
    "\n",
    "with open(url,\"r\") as f:\n",
    "    r = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    # print(soup)\n",
    "\n",
    "    # title 태그 가져오기\n",
    "    title = soup.title\n",
    "    print(f\"title {title}\")\n",
    "    print(f\"title content {title.get_text()}\")\n",
    "    print(f\"title content {title.string}\")\n",
    "    print(f\"title content {title.parent}\")\n",
    "    print(\"=\"*10)\n",
    "\n",
    "    # p 태그 가져오기\n",
    "    p1 = soup.p\n",
    "    print(f\"p {p1}\")\n",
    "    print(f\"p {p1.get_text().strip()}\") # strip() : 공백 제거\n",
    "    print(f\"p {p1.attrs}\")\n",
    "    print(f\"p {p1[\"class\"]}\")\n",
    "\n",
    "    # b 태그 가져오기\n",
    "    b1 = soup.b\n",
    "    print(f\"p {b1}\")\n",
    "    print(f\"p {b1.get_text().strip()}\")\n",
    "    print(f\"p {b1.string.strip()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p <p class=\"story\">\n",
      "      Once upon a time there were three little sisters; and their names were\n",
      "      <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>\n",
      "      ,\n",
      "      <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>\n",
      "      and\n",
      "      <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"> Tillie </a>\n",
      "      ; and they lived at the bottom of a well.\n",
      "    </p>\n",
      "p Once upon a time there were three little sisters; and their names were\n",
      "       Elsie \n",
      "      ,\n",
      "       Lacie \n",
      "      and\n",
      "       Tillie \n",
      "      ; and they lived at the bottom of a well.\n",
      "p {'class': ['story']}\n",
      "p ['story']\n"
     ]
    }
   ],
   "source": [
    "# 문서의 구조를 이용한 요소 찾기\n",
    "# parent, children, next_sibling ...\n",
    "\n",
    "url = \"./story.html\"\n",
    "\n",
    "with open(url,\"r\") as f:\n",
    "    r = f.read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    # body 의 자식들로 구조 찾기\n",
    "    # body = soup.body\n",
    "    # print(f\"body children {body.children}\")\n",
    "    # for child in body.children:\n",
    "    #     print(child)\n",
    "\n",
    "    # 첫 번째 p 요소 찾기\n",
    "    p1 = soup.p\n",
    "    p2 = p1.find_next_sibling(\"p\")\n",
    "    print(f\"p {p2}\")\n",
    "    print(f\"p {p2.get_text().strip()}\")\n",
    "    # print(f\"p {p2.string.strip()}\") # 섞여있으면 string으로 못 갖고 옴\n",
    "    print(f\"p {p2.attrs}\")\n",
    "    print(f\"p {p2[\"class\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head>\n",
      "<title>The Dormouse's story</title>\n",
      "</head>\n",
      "<p class=\"title\">\n",
      "<b> The Dormouse's story </b>\n",
      "</p>\n",
      "<p class=\"story\">\n",
      "      Once upon a time there were three little sisters; and their names were\n",
      "      <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>\n",
      "      ,\n",
      "      <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>\n",
      "      and\n",
      "      <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"> Tillie </a>\n",
      "      ; and they lived at the bottom of a well.\n",
      "    </p>\n",
      "[<p class=\"story\">...</p>]\n",
      "====================\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>\n",
      "http://example.com/tillie\n",
      "====================\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> Elsie </a>\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> Lacie </a>\n"
     ]
    }
   ],
   "source": [
    "# find() : 조건을 만족하는 요소 한 개 찾기\n",
    "# find_all() : 조건을 만족하는 요소 모두 찾기\n",
    "\n",
    "url = \"./story.html\"\n",
    "\n",
    "with open(url,\"r\") as f:\n",
    "    r = f.read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    head = soup.find(\"head\")\n",
    "    print(head)\n",
    "    \n",
    "    # p1 = soup.find(\"p\")\n",
    "    # print(p1)\n",
    "    \n",
    "    p1 = soup.find(\"p\", attrs={\"class\":\"title\"})\n",
    "    print(p1)\n",
    "\n",
    "    # p2 = soup.find(\"p\", attrs={\"class\":\"title\"})\n",
    "    p2 = soup.find(\"p\", class_=\"story\")\n",
    "    print(p2)\n",
    "\n",
    "    p_all = soup.find_all(\"p\", class_=\"story\")\n",
    "    # print(p_all)\n",
    "    print([p_all[1]])\n",
    "\n",
    "    print(\"=\"*20)\n",
    "    # a1 = soup.find(\"a\", attrs = {\"id\":\"link1\"})\n",
    "    a1 = soup.find(\"a\", id = \"link1\")\n",
    "    print(a1)\n",
    "\n",
    "    a3 = soup.find(\"a\", id = \"link3\")\n",
    "    print(a3[\"href\"])\n",
    "\n",
    "    print(\"=\"*20)\n",
    "    a_tags = soup.find_all(\"a\", limit=2)\n",
    "    for ele in a_tags:\n",
    "        print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elsie', 'Lacie', 'Tillie']\n",
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
     ]
    }
   ],
   "source": [
    "url = \"./story.html\"\n",
    "\n",
    "with open(url,\"r\") as f:\n",
    "    r = f.read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    # link1 = soup.find_all(string=\"Elsie\")\n",
    "    link1 = soup.find_all(string=[\"Elsie\",\"Lacie\",\"Tillie\"])\n",
    "    link2 = soup.find_all(\"a\", string=[\"Elsie\",\"Lacie\",\"Tillie\"])\n",
    "\n",
    "    print(link1)\n",
    "    print(link2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna SchererEmpress Marya\n",
      "FedorovnaPrince Vasili KuraginAnna PavlovnaSt. Petersburgthe princeAnna PavlovnaAnna Pavlovnathe princethe princethe princePrince VasiliAnna PavlovnaAnna Pavlovnathe princeWintzingerodeKing of Prussiale Vicomte de MortemartMontmorencysRohansAbbe Moriothe Emperorthe princePrince VasiliDowager Empress Marya Fedorovnathe baronAnna Pavlovnathe Empressthe EmpressAnna Pavlovna'sHer MajestyBaron\n",
      "FunkeThe princeAnna\n",
      "Pavlovnathe EmpressThe princeAnatolethe princeThe princeAnna\n",
      "PavlovnaAnna PavlovnaWell, Prince, so Genoa and Lucca are now just family estates of the\n",
      "Buonapartes. But I warn you, if you don't tell me that this means war,\n",
      "if you still try to defend the infamies and horrors perpetrated by\n",
      "that Antichrist- I really believe he is Antichrist- I will have\n",
      "nothing more to do with you and you are no longer my friend, no longer\n",
      "my 'faithful slave,' as you call yourself! But how do you do? I see\n",
      "I have frightened you- sit down and tell me all the news.\n",
      "If you have nothing better to do, Count [or Prince], and if the\n",
      "prospect of spending an evening with a poor invalid is not too\n",
      "terrible, I shall be very charmed to see you tonight between 7 and 10-\n",
      "Annette Scherer.\n",
      "Heavens! what a virulent attack!\n",
      "First of all, dear friend, tell me how you are. Set your friend's\n",
      "mind at rest,\n",
      "Can one be well while suffering morally? Can one be calm in times\n",
      "like these if one has any feeling?\n",
      "You are\n",
      "staying the whole evening, I hope?\n",
      "And the fete at the English ambassador's? Today is Wednesday. I\n",
      "must put in an appearance there,\n",
      "My daughter is\n",
      "coming for me to take me there.\n",
      "I thought today's fete had been canceled. I confess all these\n",
      "festivities and fireworks are becoming wearisome.\n",
      "If they had known that you wished it, the entertainment would\n",
      "have been put off,\n",
      "Don't tease! Well, and what has been decided about Novosiltsev's\n",
      "dispatch? You know everything.\n",
      "What can one say about it?\n",
      "What has been decided? They have decided that\n",
      "Buonaparte has burnt his boats, and I believe that we are ready to\n",
      "burn ours.\n",
      "Oh, don't speak to me of Austria. Perhaps I don't understand\n",
      "things, but Austria never has wished, and does not wish, for war.\n",
      "She is betraying us! Russia alone must save Europe. Our gracious\n",
      "sovereign recognizes his high vocation and will be true to it. That is\n",
      "the one thing I have faith in! Our good and wonderful sovereign has to\n",
      "perform the noblest role on earth, and he is so virtuous and noble\n",
      "that God will not forsake him. He will fulfill his vocation and\n",
      "crush the hydra of revolution, which has become more terrible than\n",
      "ever in the person of this murderer and villain! We alone must\n",
      "avenge the blood of the just one.... Whom, I ask you, can we rely\n",
      "on?... England with her commercial spirit will not and cannot\n",
      "understand the Emperor Alexander's loftiness of soul. She has\n",
      "refused to evacuate Malta. She wanted to find, and still seeks, some\n",
      "secret motive in our actions. What answer did Novosiltsev get? None.\n",
      "The English have not understood and cannot understand the\n",
      "self-abnegation of our Emperor who wants nothing for himself, but only\n",
      "desires the good of mankind. And what have they promised? Nothing! And\n",
      "what little they have promised they will not perform! Prussia has\n",
      "always declared that Buonaparte is invincible, and that all Europe\n",
      "is powerless before him.... And I don't believe a word that Hardenburg\n",
      "says, or Haugwitz either. This famous Prussian neutrality is just a\n",
      "trap. I have faith only in God and the lofty destiny of our adored\n",
      "monarch. He will save Europe!\n",
      "I think,\n",
      "None\n",
      "In a moment. A propos,\n",
      "None\n",
      "I shall be delighted to meet them,\n",
      "But tell me,\n",
      "is it true that the Dowager Empress wants Baron Funke\n",
      "to be appointed first secretary at Vienna? The baron by all accounts\n",
      "is a poor creature.\n",
      "Baron Funke has been recommended to the Dowager Empress by her\n",
      "sister,\n",
      "Now about your family. Do you know that since your daughter came\n",
      "out everyone has been enraptured by her? They say she is amazingly\n",
      "beautiful.\n",
      "I often think,\n",
      "None\n",
      "Two such charming children. And really you appreciate\n",
      "them less than anyone, and so you don't deserve to have them.\n",
      "I can't help it,\n",
      "Lavater would have said I\n",
      "lack the bump of paternity.\n",
      "Don't joke; I mean to have a serious talk with you. Do you know I\n",
      "am dissatisfied with your younger son? Between ourselves\n",
      "he was mentioned at Her\n",
      "Majesty's and you were pitied....\n",
      "What would you have me do?\n",
      "You know I did all\n",
      "a father could for their education, and they have both turned out\n",
      "fools. Hippolyte is at least a quiet fool, but Anatole is an active\n",
      "one. That is the only difference between them.\n",
      "And why are children born to such men as you? If you were not a\n",
      "father there would be nothing I could reproach you with,\n",
      "I am your faithful slave and to you alone I can confess that my\n",
      "children are the bane of my life. It is the cross I have to bear. That\n",
      "is how I explain it to myself. It can't be helped!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://pythonscraping.com/pages/warandpeace.html\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    # 등장인물 출력\n",
    "    # names = soup.find_all(\"span\", attrs={\"class\":\"green\"})\n",
    "    names = soup.find_all(\"span\", class_=\"green\")\n",
    "    for name in names:\n",
    "        print(name.string, end=\"\")\n",
    "\n",
    "    # 대사 출력\n",
    "    dialogues = soup.find_all(\"span\", class_=\"red\")\n",
    "    for d in dialogues:\n",
    "        print(d.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목 : \"엔비디아 'HBM 테스트' 통과 못해\"…삼성 \"순조롭게 진행 중\"(종합)\n",
      "작성자 : 조소영 기자 김성식 기자 김재현 기자\n",
      "작성날짜와 시간 : 2024. 5. 24. 11:11\n",
      "첫번째 문단 (서울=뉴스1) 조소영 김성식 김재현 기자 = 삼성전자의 최신 고대역폭메모리(HBM) 반도체가 발열과 전력소비 문제로 미국 엔비디아의 그래픽처리장치(GPU) 납품 테스트를 아직 통과하지 못했다는 보도가 나왔다. 삼성전자는 이에 \"테스트를 순조롭게 진행 중\"이라고 반박했다.\n",
      "(서울=뉴스1) 조소영 김성식 김재현 기자 = 삼성전자의 최신 고대역폭메모리(HBM) 반도체가 발열과 전력소비 문제로 미국 엔비디아의 그래픽처리장치(GPU) 납품 테스트를 아직 통과하지 못했다는 보도가 나왔다. 삼성전자는 이에 \"테스트를 순조롭게 진행 중\"이라고 반박했다.\n",
      "로이터 통신은 24일 해당 사안에 정통한 익명의 소식통 3명을 인용해 \"삼성전자가 미국 반도체 업체 엔비디아에 HBM을 납품하기 위한 테스트를 통과하지 못했다\"고 보도했다.\n",
      "소식통들은 삼성전자가 지난해부터 4세대 HBM인 'HBM3'와 5세대 HBM인 'HBM3E'를 엔비디아에 납품하는 테스트를 통과하기 위해 노력해왔다고 전했다. 그러나 지난 4월 삼성전자의 8층·12층짜리 HBM3E가 발열 및 전력소비와 관련한 엔비디아의 기준을 충족하지 못했다는 테스트 결과를 받았다고 언급했다.\n",
      "삼성전자 경쟁사인 SK하이닉스는 2022년 6월부터 엔비디아에 HBM3를 납품하고 있고, 미국 마이크론도 엔비디아와 HBM3E 납품 계약을 체결한 상태다. 따라서 이번 테스트 탈락으로 HBM 후발주자인 삼성전자가 관련 시장에서 뒤처질 수 있다는 우려가 커졌다는 게 소식통들의 진단이다.\n",
      "본래 게임과 영상에 활용되던 GPU는 방대한 양의 데이터를 한꺼번에 처리할 수 있어 인공지능(AI) 연산에 필수적인 고성능 반도체로 급부상했다. 지난해 생성형 AI 열풍으로 GPU 수요도 급증했는데, 엔비디아는 현재 전세계 GPU 시장의 80%를 독점하고 있다.\n",
      "HBM은 메모리 반도체인 D램을 수직으로 쌓아 올려 공간을 절약하고 전력 소비를 획기적으로 줄이는 한편 GPU가 처리한 데이터를 빠르게 저장할 수 있다.\n",
      "삼성전자는 로이터에 보낸 성명에서 \"HBM은 맞춤 제작하는 메모리 반도체로 고객사의 요구사항에 부응하는 최적화 절차가 필요하다\"며 \"고객사와 긴밀한 협력을 통해 제품을 최적화하는 과정에 있다\"고 설명했다. 엔비디아는 로이터의 논평 요청을 거부했다.\n",
      "삼성전자는 별도의 입장문을 통해서도 \"다양한 글로벌 파트너들과 HBM 공급을 위한 테스트를 순조롭게 진행 중\"이라며 \"현재 다수의 업체와 긴밀하게 협력하며 지속해서 기술과 성능을 테스트하고 있다\"고 밝혔다.\n",
      "이어 \"일부에서 제기하는 특정 시점에서의 테스트 관련 보도는 당사의 이미지와 신뢰도를 훼손할 수 있으므로 보도에 신중을 기해주시기를 바란다\"고 했다.\n",
      "cho11757@news1.kr \n"
     ]
    }
   ],
   "source": [
    "url = \"https://v.daum.net/v/20240524111151666\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    # 뉴스 제목\n",
    "    title = soup.h3\n",
    "    print(f\"제목 : {title.text}\")\n",
    "\n",
    "    # 작성자\n",
    "    writer = soup.find(\"span\", class_=\"txt_info\")\n",
    "    print(f\"작성자 : {writer.string}\")\n",
    "    \n",
    "    # 작성날짜와 시간\n",
    "    num_date = soup.find(\"span\", class_=\"num_date\")\n",
    "    print(f\"작성날짜와 시간 : {num_date.string}\")\n",
    "    \n",
    "    # 첫번째 문단 가져오기\n",
    "    para = soup.find(\"p\", attrs = {\"dmcf-ptype\":\"general\"})\n",
    "    print(f\"첫번째 문단 {para.text}\")\n",
    "    \n",
    "    # 전체 본문 내용 가져오기\n",
    "    paras = soup.find_all(\"p\", attrs={\"dmcf-ptype\":\"general\"})\n",
    "    for p in paras:\n",
    "        print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b> The Dormouse's story </b>\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n"
     ]
    }
   ],
   "source": [
    "# css select 사용\n",
    "# select() : 전체 요소 (하나만 찾아도 리스트로 반환)\n",
    "# select_one() : 한 개의 요소\n",
    "\n",
    "url = \"./story.html\"\n",
    "\n",
    "with open(url,\"r\") as f:\n",
    "    r = f.read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    title = soup.select_one(\"p.title > b\")\n",
    "    print(title)\n",
    "\n",
    "    link1 = soup.select_one(\"#link1\")\n",
    "    print(link1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "Elsie\n",
      "Elsie\n",
      "Elsie\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
      "Lacie\n",
      "Lacie\n",
      "Lacie\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
      "Tillie\n",
      "Tillie\n",
      "Tillie\n"
     ]
    }
   ],
   "source": [
    "url = \"./story.html\"\n",
    "\n",
    "with open(url,\"r\") as f:\n",
    "    r = f.read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    stories = soup.select(\"p.story > a\")\n",
    "    print(stories)\n",
    "\n",
    "    for story in stories:\n",
    "        print(story)\n",
    "        print(story.text)\n",
    "        print(story.string)\n",
    "        print(story.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"story\">\n",
      "      Once upon a time there were three little sisters; and their names were\n",
      "      <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "      ,\n",
      "      <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
      "      and\n",
      "      <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
      "      ; and they lived at the bottom of a well.\n",
      "    </p>, <p class=\"story\">...</p>]\n"
     ]
    }
   ],
   "source": [
    "url = \"./story.html\"\n",
    "\n",
    "with open(url,\"r\") as f:\n",
    "    r = f.read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "    stories = soup.select(\"p.story\")\n",
    "    # print(stories)\n",
    "\n",
    "    for story in stories:\n",
    "        temp = story.find_all(\"a\")\n",
    "\n",
    "        if temp:\n",
    "            for v in temp:\n",
    "                print(\"****\", v)\n",
    "                print(\"====\", v.string)\n",
    "        else:\n",
    "            print(\"===> \", story)\n",
    "            print(\"===> \", story.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목 : \"엔비디아 'HBM 테스트' 통과 못해\"…삼성 \"순조롭게 진행 중\"(종합)\n",
      "작성자 : 조소영 기자 김성식 기자 김재현 기자\n",
      "작성날짜와 시간 : 2024. 5. 24. 11:11\n",
      "첫번째 문단 (서울=뉴스1) 조소영 김성식 김재현 기자 = 삼성전자의 최신 고대역폭메모리(HBM) 반도체가 발열과 전력소비 문제로 미국 엔비디아의 그래픽처리장치(GPU) 납품 테스트를 아직 통과하지 못했다는 보도가 나왔다. 삼성전자는 이에 \"테스트를 순조롭게 진행 중\"이라고 반박했다.\n",
      "(서울=뉴스1) 조소영 김성식 김재현 기자 = 삼성전자의 최신 고대역폭메모리(HBM) 반도체가 발열과 전력소비 문제로 미국 엔비디아의 그래픽처리장치(GPU) 납품 테스트를 아직 통과하지 못했다는 보도가 나왔다. 삼성전자는 이에 \"테스트를 순조롭게 진행 중\"이라고 반박했다.\n",
      "로이터 통신은 24일 해당 사안에 정통한 익명의 소식통 3명을 인용해 \"삼성전자가 미국 반도체 업체 엔비디아에 HBM을 납품하기 위한 테스트를 통과하지 못했다\"고 보도했다.\n",
      "소식통들은 삼성전자가 지난해부터 4세대 HBM인 'HBM3'와 5세대 HBM인 'HBM3E'를 엔비디아에 납품하는 테스트를 통과하기 위해 노력해왔다고 전했다. 그러나 지난 4월 삼성전자의 8층·12층짜리 HBM3E가 발열 및 전력소비와 관련한 엔비디아의 기준을 충족하지 못했다는 테스트 결과를 받았다고 언급했다.\n",
      "삼성전자 경쟁사인 SK하이닉스는 2022년 6월부터 엔비디아에 HBM3를 납품하고 있고, 미국 마이크론도 엔비디아와 HBM3E 납품 계약을 체결한 상태다. 따라서 이번 테스트 탈락으로 HBM 후발주자인 삼성전자가 관련 시장에서 뒤처질 수 있다는 우려가 커졌다는 게 소식통들의 진단이다.\n",
      "본래 게임과 영상에 활용되던 GPU는 방대한 양의 데이터를 한꺼번에 처리할 수 있어 인공지능(AI) 연산에 필수적인 고성능 반도체로 급부상했다. 지난해 생성형 AI 열풍으로 GPU 수요도 급증했는데, 엔비디아는 현재 전세계 GPU 시장의 80%를 독점하고 있다.\n",
      "HBM은 메모리 반도체인 D램을 수직으로 쌓아 올려 공간을 절약하고 전력 소비를 획기적으로 줄이는 한편 GPU가 처리한 데이터를 빠르게 저장할 수 있다.\n",
      "삼성전자는 로이터에 보낸 성명에서 \"HBM은 맞춤 제작하는 메모리 반도체로 고객사의 요구사항에 부응하는 최적화 절차가 필요하다\"며 \"고객사와 긴밀한 협력을 통해 제품을 최적화하는 과정에 있다\"고 설명했다. 엔비디아는 로이터의 논평 요청을 거부했다.\n",
      "삼성전자는 별도의 입장문을 통해서도 \"다양한 글로벌 파트너들과 HBM 공급을 위한 테스트를 순조롭게 진행 중\"이라며 \"현재 다수의 업체와 긴밀하게 협력하며 지속해서 기술과 성능을 테스트하고 있다\"고 밝혔다.\n",
      "이어 \"일부에서 제기하는 특정 시점에서의 테스트 관련 보도는 당사의 이미지와 신뢰도를 훼손할 수 있으므로 보도에 신중을 기해주시기를 바란다\"고 했다.\n",
      "cho11757@news1.kr \n"
     ]
    }
   ],
   "source": [
    "# select(), select_one() 으로 변경\n",
    "\n",
    "url = \"https://v.daum.net/v/20240524111151666\"\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    # 뉴스 제목\n",
    "    title = soup.select_one(\"h3.tit_view\")\n",
    "    print(f\"제목 : {title.text}\")\n",
    "\n",
    "    # 작성자\n",
    "    writer = soup.select_one(\"span.txt_info\")\n",
    "    print(f\"작성자 : {writer.string}\")\n",
    "    \n",
    "    # 작성날짜와 시간\n",
    "    num_date = soup.select_one(\"span.num_date\")\n",
    "    print(f\"작성날짜와 시간 : {num_date.string}\")\n",
    "    \n",
    "    # 첫번째 문단 가져오기\n",
    "    para = soup.select_one(\"p[dmcf-ptype='general']\")\n",
    "    print(f\"첫번째 문단 {para.text}\")\n",
    "    \n",
    "    # 전체 본문 내용 가져오기\n",
    "    paras = soup.select(\"p[dmcf-ptype='general']\")\n",
    "    for p in paras:\n",
    "        print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML>\n",
      "<html lang=\"ko\">\n",
      "<head>\n",
      "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"/>\n",
      "    <meta name=\"viewport\" content=\"width=device-width,initial-scale=1.0,maximum-scale=1.0,minimum-scale=1.0,user-scalable=no\">\n",
      "    <meta name=\"description\" lang=\"ko\" content=\"잠시 후 다시 확인해주세요! : 네이버쇼핑\">\n",
      "    <title>에러 페이지 : 네이버쇼핑</title>\n",
      "    <link rel=\"stylesheet\" type=\"text/css\" href=\"//img.pay.naver.net/static/css/customer/naver_error.css\">\n",
      "\n",
      "    <script src=\"https://ssl.pstatic.net/static/fe/grafolio.js\"></script>\n",
      "</head>\n",
      "\n",
      "\n",
      "<body>\n",
      "<div id=\"u_skip\" class=\"u_skip\">\n",
      "    <a href=\"#content\">본문 바로가기</a>\n",
      "</div>\n",
      "<div class=\"wrap\">\n",
      "    <div class=\"header\" role=\"banner\">\n",
      "        <h1 class=\"logo\"><a href=\"//naver.com\" class=\"logo_link\"><img src=\"//img.pay.naver.net/static/images/customer/naver_logo.png\" width=\"90\" height=\"16\"\n",
      "                                                                             alt=\"네이버\"></a></h1>\n",
      "        <div class=\"nav\" role=\"navigation\">\n",
      "            <a href=\"//naver.com\" class=\"nav_link\">네이버홈</a>\n",
      "            <a href=\"//help.pay.naver.com\" class=\"nav_link\">쇼핑&페이 고객센터</a>\n",
      "        </div>\n",
      "    </div>\n",
      "    <hr>\n",
      "    <div class=\"container\" role=\"main\">\n",
      "        <div class=\"content\" id=\"content\">\n",
      "            <div class=\"image_area _errorImage\"></div>\n",
      "\n",
      "            <div class=\"info_area\">\n",
      "                <div class=\"info_txt\">\n",
      "                    <strong class=\"tit\">잠시 후 다시 확인해주세요!</strong>\n",
      "                    <p class=\"txt\">\n",
      "                        지금 이 서비스와 연결할 수 없습니다.<br>\n",
      "                        문제를 해결하기 위해 열심히 노력하고 있습니다.<br>\n",
      "                        잠시 후 다시 확인해주세요.\n",
      "                    </p>\n",
      "                </div>\n",
      "                <div class=\"info_link\">\n",
      "                    <a href=\"javascript:history.go(-1)\" class=\"link_prev\">이전 페이지</a><a href=\"//shopping.naver.com\" class=\"link_home\">네이버쇼핑 홈</a>\n",
      "                </div>\n",
      "            </div>\n",
      "        </div>\n",
      "    </div>\n",
      "    <hr>\n",
      "    <div class=\"footer\" role=\"contentinfo\">\n",
      "        <address>\n",
      "            <span>Copyright</span> ©<a href=\"http://www.navercorp.com\" class=\"link_naver\" target=\"_blank\">NAVER Corp.</a> <span>All Rights Reserved.</span>\n",
      "        </address>\n",
      "    </div>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "\n",
    "# url = \"https://shopping.naver.com/home\"\n",
    "# category id\n",
    "url = \"https://shopping.naver.com/api/modules/gnb/category?id=root&_vc_=1717171191197\"\n",
    "\n",
    "userAgent = UserAgent()\n",
    "\n",
    "headers = {\"user-agent\":userAgent.chrome}\n",
    "\n",
    "with requests.Session() as s:\n",
    "    r= s.get(url, headers=headers)\n",
    "    print(r.text)\n",
    "    # soup = BeautifulSoup(r.text,\"lxml\")\n",
    "    \n",
    "    # 1차 카테고리 가져오기\n",
    "    # cate1 = soup.select_one(\"._categoryLayer_icon_Y5zup\")\n",
    "    # print(cate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
